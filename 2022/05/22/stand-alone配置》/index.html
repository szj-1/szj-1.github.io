<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>Hexo</title><meta name="author" content="SZJ"><link rel="shortcut icon" href="/img/szj.jpg"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13.0/css/all.min.css"><meta name="generator" content="Hexo 6.2.0"></head><body><header id="page_header"><div class="header_wrap"><div id="blog_name"><a class="blog_title" id="site-name" href="/">Hexo</a></div><button class="menus_icon"><div class="navicon"></div></button><ul class="menus_items"><li class="menus_item"><a class="site-page" href="/#Publications"> Publications</a></li><li class="menus_item"><a class="site-page" href="/"> About</a></li><li class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://phower.me"> Blog</a></li></ul></div></header><main id="page_main"><div class="side-card sticky"><div class="card-wrap" itemscope itemtype="http://schema.org/Person"><div class="author-avatar"><img class="avatar-img" src="/img/szj.jpg" onerror="this.onerror=null;this.src='/img/profile.png'" alt="avatar"></div><div class="author-discrip"><h3>SZJ</h3><p class="author-bio">你就是我生命中得的一束光啊.</p></div><div class="author-links"><button class="btn m-social-links">Links</button><ul class="social-icons"><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-twitter" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-facebook-square" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-github" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-stack-overflow" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-linkedin" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-weibo" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-weixin" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-qq" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fas fa-envelope" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fas fa-rss" aria-hidden="true"></i></a></li></ul><ul class="social-links"><li><a class="e-social-link" href="/" target="_blank"><i class="fas fa-graduation-cap" aria-hidden="true"></i><span>Google Scholar</span></a></li><li><a class="e-social-link" href="/" target="_blank"><i class="fab fa-orcid" aria-hidden="true"></i><span>ORCID</span></a></li></ul></div><a class="cv-links" href="/attaches/CV.pdf" target="_blank"><i class="fas fa-file-pdf" aria-hidden="true"><span>My Detail CV.</span></i></a></div></div><div class="page" itemscope itemtype="http://schema.org/CreativeWork"><h2 class="page-title">《Spark local& stand-alone配置》</h2><article><h1 id="《Spark-local-amp-stand-alone配置》"><a href="#《Spark-local-amp-stand-alone配置》" class="headerlink" title="《Spark local&amp; stand-alone配置》"></a>《Spark local&amp; stand-alone配置》</h1><hr>
<p>title: 《Spark local&amp; stand-alone配置》<br>date: 2022-05-22 16:25:01<br>description: Spark（local）   Spark（stand-alone）</p>
<details>
<summary>阅读全文</summary>

<p>**<summary>本地模式(单机) 本地模式就是以一个独立的进程,通过其内部的多个线程来模拟整个Spark运行时环境<br>Anaconda On Linux 安装 (单台服务器脚本安装)<br>安装上传安装包: 资料中提供的Anaconda3-2021.05-Linux-x86_64.sh文件到Linux服务器上安装<br>位置在 &#x2F;export&#x2F;server:</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   cd &#x2F;export&#x2F;server<br>   #运行文件 sh Anaconda3-2021.05-Linux-x86_64.sh<br>   过程显示：<br>   …<br>   #出现内容选 yes Please answer ‘yes’ or ‘no’:’ &gt;&gt;&gt; yes …<br>   #出现添加路径：&#x2F;export&#x2F;server&#x2F;anaconda3<br>   …<br>   [&#x2F;root&#x2F;anaconda3] </p>
<blockquote>
</blockquote>
<p>   &#x2F;export&#x2F;server&#x2F;anaconda3 PREFIX&#x3D;&#x2F;export&#x2F;server&#x2F;anaconda3<br>   …</p>
</blockquote>
</blockquote>
<p>安装完成后, 退出终端， 重新进来:</p>
<blockquote>
<blockquote>
<blockquote>
<p>exit<br>   结果显示：<br>   #看到这个Base开头表明安装好了.base是默认的虚拟环境. Last login: Tue Mar 15 15:28:59 2022 from 192.168.88.1 (base)<br>   [root@node1 ~]#</p>
</blockquote>
</blockquote>
</blockquote>
<p>创建虚拟环境 pyspark 基于 python3.8</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   conda create -n pyspark python&#x3D;3.8</p>
</blockquote>
</blockquote>
<p>切换到虚拟环境内</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   conda activate pyspark 结果显示： (base) [root@node1 ~]# conda activate pyspark (pyspark) [root@node1 ~]#</p>
</blockquote>
</blockquote>
<p>在虚拟环境内安装包 （有WARNING不用管）</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   pip install pyhive pyspark jieba -i <a target="_blank" rel="noopener" href="https://pypi.tuna.tsinghua.edu.cn/simple">https://pypi.tuna.tsinghua.edu.cn/simple</a></p>
</blockquote>
</blockquote>
<p>spark 安装<br>将文件上传到 &#x2F;export&#x2F;server 里面 ，解压</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   cd &#x2F;export&#x2F;server # 解压 tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C &#x2F;export&#x2F;server&#x2F;</p>
</blockquote>
</blockquote>
<p>建立软连接</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   ln -s &#x2F;export&#x2F;server&#x2F;spark-3.2.0-bin-hadoop3.2 &#x2F;export&#x2F;server&#x2F;spark</p>
</blockquote>
</blockquote>
<p>添加环境变量</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   SPARK_HOME: 表示Spark安装路径在哪里<br>   PYSPARK_PYTHON: 表示Spark想运行Python程序, 那么去哪里找python执行器<br>   JAVA_HOME: 告知Spark Java在哪里<br>   HADOOP_CONF_DIR: 告知Spark Hadoop的配置文件在哪里<br>   HADOOP_HOME: 告知Spark Hadoop安装在哪里</p>
</blockquote>
</blockquote>
<p>   vim &#x2F;etc&#x2F;profile<br>   内容：<br>   …..<br>   注：此部分之前配置过，此部分不需要在配置<br>   #JAVA_HOME export JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk1.8.0_241 export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin export CLASSPATH&#x3D;.:$JAVA_HOME&#x2F;lib&#x2F;dt.jar:$JAVA_HOME&#x2F;lib&#x2F;tools.jar </p>
<p>   #HADOOP_HOME export HADOOP_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop-3.3.0 export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;bin:$HADOOP_HOME&#x2F;sbin </p>
<p>   #ZOOKEEPER_HOME export ZOOKEEPER_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;zookeeper export PATH&#x3D;$PATH:$ZOOKEEPER_HOME&#x2F;bin ….. </p>
<p>   #将以下部分添加进去 #SPARK_HOME export SPARK_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;spark #HADOOP_CONF_DIR export HADOOP_CONF_DIR&#x3D;$HADOOP_HOME&#x2F;etc&#x2F;hadoop #PYSPARK_PYTHON export PYSPARK_PYTHON&#x3D;&#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F;python<br>   vim .bashrc<br>   内容添加进去： </p>
<p>   #JAVA_HOME<br>   export JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk1.8.0_241<br>   #PYSPARK_PYTHON<br>   export PYSPARK_PYTHON&#x3D;&#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F;python</p>
<p>重新加载环境变量文件</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   source &#x2F;etc&#x2F;profile<br>   source ~&#x2F;.bashrc</p>
</blockquote>
</blockquote>
<p>进入 &#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F; 文件夹</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   cd &#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F;</p>
</blockquote>
</blockquote>
<p>开启 </p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>  .&#x2F;pyspark 结果显示： (base) [root@node1 bin]# .&#x2F;pyspark<br>   Python 3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0] :: Anaconda, Inc. on linux<br>   Type “help”, “copyright”, “credits” or “license” for more information.<br>   Setting default log level to “WARN”. To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). 2022-03-15 20:37:04,612 WARN util.NativeCodeLoader: Unable to load native- hadoop library for your platform… using builtin-java classes where applicable<br>    Welcome to<br>         __              __<br>    __ &#x2F; <strong>&#x2F;</strong> ___ _<em><em><strong>&#x2F; &#x2F;</strong><br>     <em>\ / _ / _ &#96;&#x2F; _<em>&#x2F; ‘</em>&#x2F;<br>     &#x2F;_</em> &#x2F; .</em><em>&#x2F;_,</em>&#x2F;</em>&#x2F; &#x2F;<em>&#x2F;_\ version 3.2.0<br>        &#x2F;</em>&#x2F;<br>    Using Python version 3.8.12 (default, Oct 12 2021 13:49:34) Spark context Web UI available at <a target="_blank" rel="noopener" href="http://node1:4040/">http://node1:4040</a> Spark context available as ‘sc’ (node1 &#x3D; local[*], app id &#x3D; local- 1647347826262). SparkSession available as ‘spark’. &gt;&gt;&gt;</p>
</blockquote>
</blockquote>
<p>查看WebUI界面</p>
<blockquote>
<blockquote>
<blockquote>
<p>浏览器访问：<br>    <a target="_blank" rel="noopener" href="http://node1:4040/">http://node1:4040/</a></p>
</blockquote>
</blockquote>
</blockquote>
<p>退出</p>
<blockquote>
<blockquote>
<blockquote>
<p>conda deactivate</p>
</blockquote>
</blockquote>
</blockquote>
<p>Standalone模式(集群) Spark中的各个角色以独立进程的形式存在,并组成Spark集群环境 Anaconda On Linux 安装 (单台服务器脚本安装 注：在 node2 和 node3 上部署)<br>安装上传安装包: 资料中提供的Anaconda3-2021.05-Linux-x86_64.sh文件到Linux服务器上安装位置在 &#x2F;export&#x2F;server:</p>
<p>cd &#x2F;export&#x2F;server # 运行文件 sh Anaconda3-2021.05-Linux-x86_64.sh</p>
<blockquote>
<blockquote>
<blockquote>
<p>过程显示：<br> …<br> #出现内容选 yes<br>  Please answer ‘yes’ or ‘no’:’<br>yes<br>   …<br>   #出现添加路径：&#x2F;export&#x2F;server&#x2F;anaconda3<br>   …<br>   [&#x2F;root&#x2F;anaconda3] &gt;&gt;&gt; &#x2F;export&#x2F;server&#x2F;anaconda3 PREFIX&#x3D;&#x2F;export&#x2F;server&#x2F;anaconda3<br>   …</p>
</blockquote>
</blockquote>
</blockquote>
<p>安装完成后, 退出终端，</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>  重新进来:<br>  exit<br>  结果显示：<br>   #看到这个Base开头表明安装好了.base是默认的虚拟环境.<br>    Last login: Tue Mar 15 15:28:59 2022 from 192.168.88.1<br>   …</p>
</blockquote>
</blockquote>
<p>在 node1 节点上把 .&#x2F;bashrc 和 profile 分发给 node2 和 node3</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   #分发 .bashrc : scp <del>&#x2F;.bashrc root@node2:</del>&#x2F; scp <del>&#x2F;.bashrc root@node3:</del>&#x2F; #分发 profile : scp &#x2F;etc&#x2F;profile&#x2F; root@node2:&#x2F;etc&#x2F; scp &#x2F;etc&#x2F;profile&#x2F; root@node3:&#x2F;etc&#x2F;<br>   …</p>
</blockquote>
</blockquote>
<p>创建虚拟环境 pyspark 基于 python3.8</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   conda create -n pyspark python&#x3D;3.8</p>
</blockquote>
</blockquote>
<p>切换到虚拟环境内</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   conda activate pyspark 结果显示： (base) [root@node1 ~]# conda activate pyspark (pyspark)<br>在虚拟环境内安装包 （有WARNING不用管）<br>    pip install pyhive pyspark jieba -i <a target="_blank" rel="noopener" href="https://pypi.tuna.tsinghua.edu.cn/simple">https://pypi.tuna.tsinghua.edu.cn/simple</a><br>    spark 安装<br>将文件上传到 &#x2F;export&#x2F;server 里面 ，解压</p>
</blockquote>
</blockquote>
<p>node1 节点节点进入 &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf 修改以下配置文件</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf</p>
</blockquote>
</blockquote>
<p>将文件 workers.template 改名为 workers，并配置文件内容</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   mv workers.template workers vim workers<br>    # localhost删除，内容追加文末： node1<br>    node2<br>    node3<br>    # 功能: 这个文件就是指示了 当前SparkStandAlone环境下, 有哪些worker</p>
</blockquote>
</blockquote>
<p>将文件 spark-env.sh.template 改名为 spark-env.sh，并配置相关内容</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   mv spark-env.sh.template spark-env.sh vim spark-env.sh</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>文末追加内容：<br>   ##设置JAVA安装目录 JAVA_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;jdk<br>   ##HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群 HADOOP_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop YARN_CONF_DIR&#x3D;&#x2F;export&#x2F;server&#x2F;hadoop&#x2F;etc&#x2F;hadoop ## 指定spark老大node1的IP和提交任务的通信端口 # 告知Spark的node1运行在哪个机器上 export SPARK_node1_HOST&#x3D;node1<br>    #告知sparknode1的通讯端口 export SPARK_node1_PORT&#x3D;7077<br>    # 告知spark node1的 webui端口 SPARK_node1_WEBUI_PORT&#x3D;8080<br>    # worker cpu可用核数 SPARK_WORKER_CORES&#x3D;1<br>    # worker可用内存 SPARK_WORKER_MEMORY&#x3D;1g<br>    # worker的工作通讯地址 SPARK_WORKER_PORT&#x3D;7078<br>    # worker的 webui地址 SPARK_WORKER_WEBUI_PORT&#x3D;8081<br>    ## 设置历史服务器 # 配置的意思是 将spark程序运行的历史日志 存到hdfs的&#x2F;sparklog文件夹中 SPARK_HISTORY_OPTS&#x3D;”- Dspark.history.fs.logDirectory&#x3D;hdfs:&#x2F;&#x2F;node1:8020&#x2F;sparklog&#x2F; - Dspark.history.fs.cleaner.enabled&#x3D;true”</p>
</blockquote>
</blockquote>
</blockquote>
<p>开启 hadoop 的 hdfs 和 yarn 集群</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<pre><code>start-dfs.sh 
start-yarn.sh
</code></pre>
<p>在HDFS上创建程序运行历史记录存放的文件夹，同样 conf 文件目录下:</p>
<blockquote>
</blockquote>
<p>   hadoop fs -mkdir &#x2F;sparklog hadoop fs -chmod 777 &#x2F;sparklog</p>
</blockquote>
</blockquote>
<p>将 spark-defaults.conf.template 改为 spark-defaults.conf 并做相关配置</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   mv spark-defaults.conf.template spark-defaults.conf vim spark-defaults.conf 文末追加内容为： # 开启spark的日期记录功能 spark.eventLog.enabled true # 设置spark日志记录的路径 spark.eventLog.dir hdfs:&#x2F;&#x2F;node1:8020&#x2F;sparklog&#x2F; # 设置spark日志是否启动压缩 spark.eventLog.compress true</p>
</blockquote>
</blockquote>
<p>配置 log4j.properties 文件 将文件第 19 行的 log4j.rootCategory&#x3D;INFO, console 改为<br>log4j.rootCategory&#x3D;WARN, console （即将INFO 改为 WARN 目的：输出日志, 设置级别为<br>WARN 只输出警告和错误日志，INFO 则为输出所有信息，多数为无用信息）</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   mv log4j.properties.template log4j.properties vim log4j.properties 结果显示：<br>    …<br>    18 # Set everything to be logged to the console<br>    19 log4j.rootCategory&#x3D;WARN, console ….</p>
</blockquote>
</blockquote>
<p>node1 节点分发 spark 安装文件夹 到 node2 和 node3 上</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   node1 节点分发 spark 安装文件夹 到 node2 和 node3 上</p>
</blockquote>
</blockquote>
<p>在node2 和 node3 上做软连接</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   ln -s &#x2F;export&#x2F;server&#x2F;spark-3.2.0-bin-hadoop3.2 &#x2F;export&#x2F;server&#x2F;spark</p>
</blockquote>
</blockquote>
<p>重新加载环境变量</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   source &#x2F;etc&#x2F;profile</p>
</blockquote>
</blockquote>
<p>进入 &#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin 文件目录下 启动 start-history-server.sh</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   cd &#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin .&#x2F;start-history-server.sh</p>
</blockquote>
</blockquote>
<p>访问 WebUI 界面</p>
<blockquote>
<blockquote>
<blockquote>
</blockquote>
<p>   浏览器访问： <a target="_blank" rel="noopener" href="http://node1:18080/">http://node1:18080/</a></p>
</blockquote>
</blockquote>
<p>readme.md<br>**</p>
</details>

<hr>
</article></div></main><div class="nav-wrap"><div class="nav"><button class="site-nav"><div class="navicon"></div></button><ul class="nav_items"><li class="nav_item"><a class="nav-page" href="/#Publications"> Publications</a></li><li class="nav_item"><a class="nav-page" href="/"> About</a></li><li class="nav_item"><a class="nav-page" target="_blank" rel="noopener" href="https://phower.me"> Blog</a></li></ul></div><div class="cd-top"><i class="fa fa-arrow-up" aria-hidden="true"></i></div></div><footer id="page_footer"><div class="footer_wrap"><div class="copyright">&copy;2020 - 2022 by SZJ</div><div class="theme-info">Powered by <a target="_blank" href="https://hexo.io" rel="nofollow noopener">Hexo</a> & <a target="_blank" href="https://github.com/PhosphorW/hexo-theme-academia" rel="nofollow noopener">Academia Theme</a></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery-pjax@latest/jquery.pjax.min.js"></script><script src="/js/main.js"></script></body></html>